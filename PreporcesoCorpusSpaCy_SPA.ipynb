{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PreporcesoCorpusSpaCy_SPA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aksil-2013/tic-Y8289446F/blob/main/PreporcesoCorpusSpaCy_SPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDSHfQfEjBfJ"
      },
      "source": [
        "# Descargar, instalar y utilizar SpaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx6MApnejFUy"
      },
      "source": [
        "Una vez que tenemos ya abierto un cuaderno en Colab ya podemos utilizar Python.\n",
        "\n",
        "Para realizar el análisis computacional de un texto, sin embargo, es necesario instalar módulos específicos. En este caso vamos a instalar SpaCy.\n",
        "\n",
        "Tres son los pasos que vamos a realizar:\n",
        "\n",
        "* importarlo SpaCy (para que esté activo desde Python),\n",
        "* descargar el módulo de idioma (español en este caso) y\n",
        "* activar el módulo de idioma.\n",
        "\n",
        "En este caso vamos a trabajar con un corpus en español. Las opciones para un corpus en inglés están en el otro cuaderno.\n",
        "\n",
        "Todo esto se realiza con el siguiente código. Incluyo un comentario de lo que hace cada línea (los comentarios empiezan con el símbolo \"#\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1t3hr38kATv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c41f3aa-56b3-4ecf-e9fa-0b4e0df790fa"
      },
      "source": [
        "import spacy #Importa SpaCy para que se pueda ejecutar desde Python.\n",
        "\n",
        "!python -m spacy download es_core_news_sm #Descarga módulos SpaCy para español.\n",
        "\n",
        "import es_core_news_sm\n",
        "nlp_esp = es_core_news_sm.load() #Importa y carga el analizador del español en la variable \"nlp_esp\"."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.2 MB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Building wheels for collected packages: es-core-news-sm\n",
            "  Building wheel for es-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for es-core-news-sm: filename=es_core_news_sm-2.2.5-py3-none-any.whl size=16172933 sha256=d07fef9d671b25e6ae31881bf3437d899992202c816981ac2ea837c054f3f33f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6an9jnpt/wheels/21/8d/a9/6c1a2809c55dd22cd9644ae503a52ba6206b04aa57ba83a3d8\n",
            "Successfully built es-core-news-sm\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Av4AkUxSVh99"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKVouMTSkwE8"
      },
      "source": [
        "A partir de este momento, los módulos de PLN para analizar textos en español están en la variable \"nlp_esp\". Para analizar un texto hay que llamar (mediante órdenes Python que luego veremos) a esta varible. Pero antes necesitamos cargar el corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1moah9Pu6LW"
      },
      "source": [
        "# Cargar y abrir un fichero de texto desde Colab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtItkUQ3vIkV"
      },
      "source": [
        "Al ser una herramienta en la nube, una de las desventajas de usar Colab es que no tenemos un acceso fácil a los textos de nuestra máquina.\n",
        "\n",
        "Aquí se expone un método sencillo para subir a Colab un documento de texto que será nuestro corpus y cómo introducirlo en una variable para poder procesarlo en Python. Subir un directorio completo con varios textos es más complejo. Por ahora haremos el preproceso de un solo texto.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1aS-uRd0BT5"
      },
      "source": [
        "## Cargar el corpus.\n",
        "\n",
        "El corpus será por ahora un único documento. Debe estar en nuestro ordenador. En principio será un texto sencillo (*plain text*) codificado en UTF-8.\n",
        "\n",
        "Para cargarlos en nuestro cuaderno Colab, utilizamos el siguiente código. Una vez ejecutado aparecerá un pequeño formulario para subir el fichero de texto.\n",
        "\n",
        "> Más fácil: utiliza la opción de subir fichero de Google Colab que tienes en el menú de la izquierda :-)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "NuT2uEBy2JTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1glZEJHjqSIs"
      },
      "source": [
        "## Abrir el documento: asignarlo a una variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMJcFIB33UnS"
      },
      "source": [
        "Una vez subido el texto a Colab, debemos asignarlo a una varialbe. La variable representará a partir de ahora todo el corpus y de esta manera podremos procesarlo en Python.\n",
        "\n",
        "Para ello utilizamos el siguiente código. La primera línea abre (\"open\") el fichero que se ha subido antes y lo lee (\"read\"), es decir, transforma el fichero en cadena de caracteres que se pueda procesar. La segunda línea simplemente muestra en la pantalla (\"print\") los cien primeros caracteres para comprobar que el fichero se ha cargado sin problemas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HSs8gwD2voB"
      },
      "source": [
        "corpus = open(\"trafalgar1.txt\", \"r\").read() #Abre el fichero de texto y lo lee (es decir, lo transforma en una cadena de caracteres).\n",
        "print(corpus[:100]) #Muestra en pantalla los cien primeros caracteres solo para comprobar que va todo bien.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rr71KYSdl7H"
      },
      "source": [
        "(NOTA febrero 2021: para subir un corpus de varios ficheros, ver aquÍÍ: https://rozbeh.github.io/colab_101.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U41j_OS5qmem"
      },
      "source": [
        "# Analizar el texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afBw6f8EqpSg"
      },
      "source": [
        "Una vez cargado SpaCy y nuestro texto, ya podemos proceder a su análisis.\n",
        "\n",
        "El análisis es bastante sencillo. Basta con escribir esto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALfLdxoo9Sqd"
      },
      "source": [
        "analisis = nlp_esp(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVmB_e5O97ka"
      },
      "source": [
        "\"nlp_esp()\" es el analizador que hemos cargado al principio.\n",
        "\n",
        "Mediante la expresión \"nlp_esp(corpus)\" indicamos a Pyhon que con el analizador antes cargado analice el corpus contenido en la variable \"corpus\".\n",
        "\n",
        "Finalmente, el análisis se almacena en la variable \"analisis\".\n",
        "\n",
        "Si ha ido todo bien, no aparecerá nada en la pantalla. En las siguientes secciones veremos cómo mostrar y extraer los análisis realizados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve-Tc3Db-exX"
      },
      "source": [
        "## Mostrar el texto analizado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x55PJ0l-hcN"
      },
      "source": [
        "Para mostrar el análisis realizado debemos crear un bucle que vaya exrayendo cada palabra analizada de la variable y lo vaya mostrando en pantalla.\n",
        "\n",
        "De los diferentes análisis que hace SpaCy, vamos a ver:\n",
        "\n",
        "* lemas y categorías gramaticales,\n",
        "* grupos sintáticos (*chunks*) y\n",
        "* entidades."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7asjgcqH_Pz4"
      },
      "source": [
        "### Mostrar palabra, lema y categoría gramatical\n",
        "\n",
        "Vamos a extraer del análisis cada palabra (opción \"token\"), su lema (opción \"token.lemma_\") y su categoría gramatical (opción \"token.pos_\").\n",
        "\n",
        "Para que el código funcione bien es muy importante respetar cómo está escrito: la tabulación, los paréntesis, los puntos, etc. Cualquier cambio en esto puede hacer que el código no funcione.\n",
        "\n",
        "En este caso \"for\" establece el bucle. La primera línea se leería: \"por cada token en la variable 'analisis'\" (que es donde está el corpus analizado). Esta línea debe acabar en dos puntos \":\".\n",
        "\n",
        "La segunda línea indica qué debe hacer por cada token. En este caso, mostrar en pantalla (\"print\") el token, su lema y su categoría gramatical. Esta línea tiene que estar tabulada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dgx_wrx_T1K"
      },
      "source": [
        "for token in analisis:\n",
        "  print(token, token.lemma_, token.pos_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofEr5jjo_WTf"
      },
      "source": [
        "Simplemente está mostrando en pantalla el análisis de tokens, lemas y categorías gramaticales (comete al menos un error, ¿lo veis?).\n",
        "\n",
        "Esto, así como está ahora, no nos sirve de mucho. Para poder utilizarlo luego en, por ejemplo, Stylo o para analizarlo con AntConc necesitamos generar un nuevo fichero de texto con el corpus analizado. Para ello se realiza el siguiente proceso:\n",
        "\n",
        "1. creamos una variable vacía con formato cadena. El formato cadena es el texto plano. Aquí guardaremos el corpus analizado en formato texto para luego poder descargarlo.\n",
        "\n",
        "2. repetimos el bucle de antes pero ahora, en vez de mostrarlo en pantalla, damos la orden de que, primero, pase la información del análisis a cadena de texto y, segundo, lo guarde en la variable anterior.\n",
        "\n",
        "Para comprobar que está todo bien, daremos también la orden de que imprima en pantalla los 200 primeros caracteres:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpKw8I3GJYyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b64df0-afcb-4233-cd95-70b77a12b8a1"
      },
      "source": [
        "corpus_pos = '' #Variable vacía con formato \"cadena\" (es decir, texto sencillo) para guardar el corpus analizado.\n",
        "\n",
        "for token in analisis: #inicio de bucle\n",
        "  palabra = str(token) #convierte \"token\" a cadena de texto y lo guarda en \"palabra\"\n",
        "  lema = str(token.lemma_) #Idem con el lema\n",
        "  cat = str(token.pos_) #Idem con la categoría gramatical.\n",
        "  corpus_pos += palabra+\"\\t\"+lema+\"\\t\"+cat+\"\\n\" #Con \"+=\" se va introduciendo en la variable anterior el análisis de cada palabra.\n",
        "                                                #\"\\t\" indica que se separe con un tabulador y \"\\n\" (cambio de línea) que introduzca un token por línea.\n",
        "\n",
        "print(corpus_pos[:200]) #Muestra en pantalla los 200 primeros caracteres analizados para comprobar que está todo bien."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#\t#\tPROPN\n",
            "Benito\tBenito\tPROPN\n",
            "Pérez\tPérez\tPROPN\n",
            "Galdós\tGaldós\tPROPN\n",
            "\n",
            "\t\n",
            "\tSPACE\n",
            "#\t#\tPROPN\n",
            "Trafalgar\tTrafalgar\tPROPN\n",
            "\n",
            "\t\n",
            "\tSPACE\n",
            "#\t#\tPROPN\n",
            "Biblioteca\tBiblioteca\tPROPN\n",
            "Virtual\tVirtual\tPROPN\n",
            "Migue\tMigue\tPROP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KCAsk3y-deZ"
      },
      "source": [
        "Por ahora va todo bien.\n",
        "\n",
        "Como veis, cada línea es el análisis de un *token* (incluido el espacio). Cada *token* tiene asociado su lema (segunda columna) y su categoría gramatical (tercera columna).\n",
        "\n",
        "Las etiquetas categoriales y su significado lo podéis ver aquí:\n",
        "\n",
        "<https://spacy.io/api/annotation#pos-tagging>\n",
        "\n",
        "Esto es lo mismo que nos ofrece Freeling o Contawords, herramientas que vimos en asignaturas anteriores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cvSWzvUjO_b"
      },
      "source": [
        "## Guardar el análisis en un fichero de texto y decargarlo\n",
        "\n",
        "Si ya tenemos el análisis de token, lema y categoría gramatical en una variable texto (fomrato cadena), solo queda guardar el contenido de la variable en un fichero y descargarlo.\n",
        "\n",
        "Para ello... (en comentario se explica lo que hace cada línea):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vy37mYGXk2ir",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d22865b8-cc6c-40b4-cfd9-acbdce0e9f37"
      },
      "source": [
        "salida = open('analisis_pos.csv', 'w') #Se crea un fichero de texto nuevo con opción de escritura (\"w\") y se asigna a la variable \"salida\".\n",
        "salida.write(corpus_pos) #La opción \"write\" escribe el texto de la variable \"corpus_pos\" en el fichero creado.\n",
        "salida.close() #Se cierra el fichero\n",
        "\n",
        "files.download('analisis_pos.csv') #Orden de descarga del fichero creado."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_efef4ba4-2107-4f9c-a84f-d4a5f57b9108\", \"analisis_pos.csv\", 909939)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IfCzHoembnu"
      },
      "source": [
        "## Extraer solo lemas\n",
        "\n",
        "Una opción muy común en análisis de corpus es analizar éste solo con los lemas. De esta manera, como sabéis, los problemas derivados de la flexión morfológica (sobre todo la flexión verbal) se ignoran. Con el corpus lematizado podemos, por ejemplo, extraer una lista de frecuencias más real o podemos comparar textos con Stylo sin tener en cuenta variaciones morfológicas.\n",
        "\n",
        "El siguiente código repite lo que hemos hecho antes pero, en vez de guardar *token*, lema y categoría gramatical, guarda solo los lemas.\n",
        "\n",
        "Partimos de que el corpus está ya analizado en la variable \"analisis\". El proceso es el siguiente:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL4pnIz0nOkD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "eb2d783e-38b0-4f0f-f669-d0b952a1b160"
      },
      "source": [
        "corpus_lemas = '' #Variable vacía con formato \"cadena\" (es decir, texto sencillo) para guardar los lemas.\n",
        "\n",
        "for token in analisis: #inicio de bucle\n",
        "  lema = str(token.lemma_) #Extrae el lema de cada token y lo pasa a cadena de texto\n",
        "  corpus_lemas += lema+\" \" #Introduce en la variable corpus_lemas el lema de cada palabra y los separa con un espacio en blanco.\n",
        "print(corpus_lemas[:200]) #Muestra en pantalla los 200 primeros caracteres analizados para comprobar que está todo bien.\n",
        "\n",
        "salida = open('analisis_lemas.txt', 'w') #Genera fichero, lo escribe, guarda y descarga.\n",
        "salida.write(corpus_lemas)\n",
        "salida.close()\n",
        "\n",
        "files.download('analisis_lemas.txt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Benito Pérez Galdós \n",
            " Trafalgar \n",
            "\n",
            " Se me permitir que antes de referir el gran suceso de que ser testigo , decir alguno palabra sobrar mi infancia , explicar por qué extraño manera me llevar lo azarar\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6f863942-adfe-4026-acd0-8958d93326b8\", \"analisis_lemas.txt\", 1309)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csRGb_juoPHC"
      },
      "source": [
        "Como veis, el código es similar al anterior. Cambian las variables y que, en este caso, solo nos quedamos con los lemas. El resultado es el mismo corpus que hemos cargado al principio pero formado solo por los lemas (en vez de las palabras flexionadas)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyF5rkZfoeeE"
      },
      "source": [
        "## Extraer solo nombres\n",
        "\n",
        "Otra preproceso común en lingüística de corpus es analizar un corpus solo por las palabras con significado léxico (nombre, verbos, adjetivos y adverbios) y filtrar las palabras con significado gramatical. Para análisis temáticos (que se verán en otro tema) es una práctica común.\n",
        "\n",
        "A modo de prueba, el siguiente código extraerá del corpus analizado solo los nombres comunes. El código es similar al anterior, pero se va a inroducir un condicional. Los condicionales empiezan siempre por \"if\". Con esto daremos la orden de que se incluyan en la variable solo aquellas palabras (condicional \"if\") cuya categoría gramatical sea nombre:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY_ZH6CEpWRy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0d2c5195-2254-4863-849a-e08af53cbe49"
      },
      "source": [
        "corpus_soloNombres = '' #Variable vacía con formato \"cadena\" (es decir, texto sencillo) para guardar los lemas.\n",
        "\n",
        "for token in analisis: #inicio de bucle\n",
        "  if token.pos_ == \"NOUN\": #Condicional: solo aquellos *tokens* cuya categoría gramatical sea \"NOUN\".\n",
        "    nombre = str(token.lemma_) #Extrae el lema de cada nombre y lo pasa a cadena de texto\n",
        "    corpus_soloNombres += nombre+\" \" #Introduce en la variable corpus_soloNombres el lema de cada nombre y los separa con un espacio en blanco.\n",
        "print(corpus_soloNombres[:200]) #Muestra en pantalla los 200 primeros caracteres analizados para comprobar que está todo bien.\n",
        "\n",
        "salida = open('analisis_soloNombres.txt', 'w') #Crea fichero, etc.\n",
        "salida.write(corpus_soloNombres)\n",
        "salida.close()\n",
        "\n",
        "files.download('analisis_soloNombres.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "suceso testigo palabra infancia manera azarar vida catástrofe marino nacimiento partir hecho vida parentela vez descendiente partir librar apellido ser madre tiempo noticiar ascendiente parentesco Doy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_70796158-baaf-402d-abe1-4451dbabac23\", \"analisis_soloNombres.txt\", 328)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-t7k3Mhq0e7"
      },
      "source": [
        "El fichero resultante es por tanto solo el lema de los nombres del corpus.\n",
        "\n",
        "Podríamos hacer lo mismo con los verbos (\"VERB\"), los adjetivos (\"ADJ\") o incluso solo con preposiones, determinantes, etc. Y en vez de lemas se podría guardar el *token* de cada nombre.\n",
        "\n",
        "Si se quisiera un análisis más fino (por tipos de nombres, o tipos de verbo), en vez de extraer la categoría gramatical con \"token.pos_\", habría que usar la opción \"token.tag_\". Cómo interpretar cada etiqueta se puede consultar aquí: <https://spacy.io/api/annotation#pos-tagging>\n",
        "\n",
        "Por ejemplo, en atribución de autoría hay una hipótesis que dice que el estilo de una persona viene determinado por cómo y cuánto utiliza las palabras con significado gramatical: preposiciones, determinantes, etc.\n",
        "\n",
        "El siguiente ejemplo crea un corpus formado solo por las preposiciones (\"ADP\"), conjunciones (\"CONJ\"), determinantes (\"DET\") o pronombres (\"PRON\"):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K2nugL4tbyK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e453380c-7bbc-4b12-e11f-2bf797f90ad5"
      },
      "source": [
        "corpus_soloParticulas = '' #Variable vacía con formato \"cadena\" (es decir, texto sencillo) para guardar los lemas.\n",
        "categorias = (\"ADP\", \"CONJ\", \"CCONJ\", \"SCONJ\", \"DET\", \"PRON\") #Ponemos en esta variable las categorías gramaticales que queremos incluir en el corpus final.\n",
        "\n",
        "for token in analisis: #inicio de bucle\n",
        "  if token.pos_ in categorias: #Condicional: comprueba que la categoría del *tokens* está en la lista de categorías de la variable \"categorías\".\n",
        "    lema = str(token.lemma_) #Extrae el lema y lo pasa a cadena de texto\n",
        "    corpus_soloParticulas += lema+\" \" #Introduce en la variable corpus_soloParticulas el lema de cada palabra y los separa con un espacio en blanco.\n",
        "print(corpus_soloParticulas[:200]) #Muestra en pantalla los 200 primeros caracteres analizados para comprobar que está todo bien.\n",
        "\n",
        "salida = open('analisis_soloParticulas.txt', 'w')\n",
        "salida.write(corpus_soloParticulas)\n",
        "salida.close()\n",
        "\n",
        "files.download('analisis_soloParticulas.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Se me que de el de que alguno sobrar mi por qué me lo de lo a lo de nuestro Al de mi a lo de lo que de su propio quien su los por el si se del mismo de Yo en este mi con y de mi a quien por poco de ni\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_8fc5e49b-3ad7-40d5-a384-54f0f21a2dec\", \"analisis_soloParticulas.txt\", 391)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX4C6IRaum1P"
      },
      "source": [
        "Si, por el contrario, quisiéramos extraer solo los nombres, verbos y adjetivos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqToYQ0QuraZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f99c3026-dc7c-4d97-b64b-bf8a401ea237"
      },
      "source": [
        "corpus_soloNVA = '' #Variable vacía con formato \"cadena\" (es decir, texto sencillo) para guardar los lemas.\n",
        "categorias = (\"NOUN\", \"VERB\", \"ADJ\") #Ponemos en esta variable las categorías gramaticales que queremos incluir en el corpus final.\n",
        "\n",
        "for token in analisis: #inicio de bucle\n",
        "  if token.pos_ in categorias: #Condicional: comprueba que la categoría del *tokens* está en la lista de categorías de la variable \"categorías\".\n",
        "    lema = str(token.lemma_) #Extrae el lema de cada token y lo pasa a cadena de texto\n",
        "    corpus_soloNVA += lema+\" \" #Introduce en la variable corpus_soloNVA el lema de cada palabra y los separa con un espacio en blanco.\n",
        "print(corpus_soloNVA[:200]) #Muestra en pantalla los 200 primeros caracteres analizados para comprobar que está todo bien.\n",
        "\n",
        "salida = open('analisis_soloNVA.txt', 'w')\n",
        "salida.write(corpus_soloNVA)\n",
        "salida.close()\n",
        "\n",
        "files.download('analisis_soloNVA.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "permitir referir gran suceso testigo decir palabra infancia explicar extraño manera llevar azarar vida presenciar terrible catástrofe marino hablar nacimiento imitar mayor partir contar hecho vida nom\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_364a63d7-5233-4999-b12d-e3c768e6ac65\", \"analisis_soloNVA.txt\", 565)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQhmC_Wgv7gV"
      },
      "source": [
        "## Extracción de grupos sintácticos.\n",
        "\n",
        "SpaCy también realiza análisis sintáticos (basado en dependencias). Extraer el árbol de dependencias necesita un poco más de código que no vamos a tratar aquí (si a alguien le interesa, está muy bien documentado en su web <https://spacy.io/usage/spacy-101#annotations-pos-deps>).\n",
        "\n",
        "En vez del árbol sintáctico entero, vamos a extraer los grupos nominales o *chunks*: grupos de palabras que forman una unidad sintática alrededor de un nombre. No siempre responden al sintagma nominal en sentido estricto, pero sí a menudo.\n",
        "\n",
        "Si en la variable \"analisis\" tenemos todo el análisis del corpus, se puede acceder a los grupos nominales con la opción \"noun_chunks\". Así: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwL1m8WWxZHQ"
      },
      "source": [
        "corpus_gruposNominales = '' #Variable vacía con formato \"cadena\" (es decir, texto sencillo) para guardar los grupos nominales.\n",
        "\n",
        "for item in analisis.noun_chunks: #inicio de bucle sobre los grupos nominales\n",
        "  grupo_nominal = str(item) #Lo pasamos a cadena de texto para poder guardarlo\n",
        "  corpus_gruposNominales += grupo_nominal+\"\\n\" #Introduce en la variable textual el grupo nominal y los separa con un cambio de línea (\\n).\n",
        "print(corpus_gruposNominales[:200]) #Muestra en pantalla los 200 primeros caracteres analizados para comprobar que está todo bien.\n",
        "\n",
        "salida = open('analisis_gruposNominales.txt', 'w')\n",
        "salida.write(corpus_gruposNominales)\n",
        "salida.close()\n",
        "\n",
        "files.download('analisis_gruposNominales.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbCGaW6Ky3gq"
      },
      "source": [
        "## Extracción de entidades\n",
        "\n",
        "SpaCy también extrae las entidades del textos: aquellos nombres que hacen referencia personas, lugares, organizaciones, etc.\n",
        "\n",
        "La lista completa de entidades que SpaCy es capaz de detectar y clasificar está aquí: <https://spacy.io/api/annotation#named-entities>.\n",
        "\n",
        "Las entidades están ya analizadas. El proceso para extraerlas de la variable donde está analizado el texto es similar a la extracción de *chunk*, pero en vez de \".noun_chunks\" utilizaremos \".ents\" de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0FL5sRTX0Xk"
      },
      "source": [
        "corpus_entidades = '' #Variable vacía con formato \"cadena\" (es decir, texto sencillo) para guardar las entidades.\n",
        "\n",
        "for ent in analisis.ents: #inicio de bucle sobre las entidades\n",
        "  entidad = ent.text #el token de la entidad (en formato cadena de texto).\n",
        "  tipo = ent.label_ #el tipo de entidad (en formato cadena de texto).\n",
        "  corpus_entidades += entidad+\" \"+tipo+\"\\n\" #Introduce en la variable corpus_entidades la entidad y su tipo, y los separa con un cambio de línea (\\n).\n",
        "\n",
        "print(corpus_entidades[:200]) #Muestra en pantalla los 200 primeros caracteres analizados para comprobar que está todo bien.\n",
        "  \n",
        "salida = open('analisis_entidades.txt', 'w')\n",
        "salida.write(corpus_entidades)\n",
        "salida.close()\n",
        "\n",
        "files.download('analisis_entidades.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX95jXh81H4e"
      },
      "source": [
        "O podríamos extraer solo las entidades persona de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyH0Rfr31Mdw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "61f2172b-21a5-4172-e7f3-26be677544f1"
      },
      "source": [
        "corpus_personas = '' #Variable vacía con formato \"cadena\" (es decir, texto sencillo) para guardar las entidades persona.\n",
        "\n",
        "for ent in analisis.ents: #inicio de bucle sobre los grupos nominales\n",
        "  if ent.label_ == \"PER\": #Condicional para seleccionar solo las entidades cuya categoría sea \"persona\".\n",
        "    entidad = ent.text #El token de la entidad en formato cadena de texto.\n",
        "    corpus_personas += entidad+\"\\n\" #Introduce cada entidad persona y las separa con un cambio de línea (\\n).\n",
        "print(corpus_personas[:200]) #Muestra en pantalla los 200 primeros caracteres analizados para comprobar que está todo bien.\n",
        "\n",
        "salida = open('analisis_entidadesPersona.txt', 'w')\n",
        "salida.write(corpus_personas)\n",
        "salida.close()\n",
        "\n",
        "files.download('analisis_entidadesPersona.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Benito Pérez Galdós\n",
            "Administración\n",
            "Emperador de Trapisonda.\n",
            "Pablos\n",
            "Dios\n",
            "Aquello\n",
            "Según\n",
            "Sólo\n",
            "Andalucía\n",
            "Alonso Gutiérrez de Cisniega\n",
            "Enseñáronme\n",
            "Sr. Don Alonso\n",
            "Lázaro\n",
            "«Gabriel\n",
            "Doña Francisca\n",
            "Ay\n",
            "Alonsito\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_04364050-1aae-4fc1-acd4-20a207d15069\", \"analisis_entidadesPersona.txt\", 6962)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21al5WEl1vHj"
      },
      "source": [
        "Como vemos, en este caso, al ser una novela, está extrayendo personajes."
      ]
    }
  ]
}